# -*- coding: utf-8 -*-
"""Updated_Final_Notebook_Simply_Business.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lh6yiqzc4XvI56sdsXuv3EsqsrvE8a_v

# ***DATA LOADING***
"""

!nvidia-smi    # ensuring whether the terminal is connected to GPU

# Importing the drive modules, giving the appropriate path where I have kept my dataset.

# Please comment out this part while running..

from google.colab import drive
drive.mount('/content/drive')

import os
os.chdir('/content/drive/MyDrive/SimplyBusiness')

# Importing all the necessary libraries required for model running...

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string
import re
import warnings

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, ConfusionMatrixDisplay,classification_report
from sklearn.metrics import roc_auc_score, precision_recall_curve, auc
from sklearn.ensemble import RandomForestClassifier

from sklearn.preprocessing import LabelEncoder
import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, Dataset
import numpy as np



warnings.filterwarnings('ignore')

from gensim.models import CoherenceModel
from gensim.corpora import Dictionary
from gensim.models.ldamodel import LdaModel


from sklearn.feature_extraction.text import TfidfVectorizer
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt, seaborn as sb

# Loading the dataset which has been scraped from the decision files..

df = pd.read_csv('updated_final_merged_df.csv')

df # Visualising the dataframe

"""# ***PREPROCESSING THE DATA***"""

# Printing the shape of the dataframe.
df.shape

# Printing the columns of the dataframe.
df.columns

# Printing out the number of unique values present in every feature column..

df.nunique()

df.isnull().sum() #  Checking for NULL values..

df.drop('extras', axis=1, inplace=True)  # Dropping the NULL column..

df.isnull().sum()

df.shape

# Find duplicate rows, keeping the first occurrence
duplicates = df.duplicated(keep='first')

# Get the duplicate rows
duplicate_rows = df[duplicates]
len(duplicate_rows)

df = df.drop_duplicates(keep='first')

df.shape

df.nunique()

df.decision_id.value_counts() # Ensuring the value counts equal to one across every deicision id present..

#  Printing the records where both first two columns have no information on them (information on what the complaint is about is absent).

df[pd.isna(df['Complaint_Info']) & pd.isna(df['Complaint_Explanation'])].shape

'''
REASONS BEHIND THE BELOW PREPROCESSING WORK

-> COMPLAINT_INFO and Complaint_Explanation are two columns which captures the first two sections from every pdf document parsed..
-> this would give us an idea on what the complaint is all about with regards to a detailed explanation..
-> We are checking for the records where there are NULL values present on both the columns...
-> as these records wont be useful in the modeling purpose and the size is way lesser as compared to the original size..
-> thus removing those records..

'''

df = df[~(pd.isna(df['Complaint_Info']) & pd.isna(df['Complaint_Explanation']))] # Removing those records as they are of no use to us..

# Filling the remaining NULL values with whitespace..

df['Complaint_Info'] = df['Complaint_Info'].fillna('')
df['Complaint_Explanation'] = df['Complaint_Explanation'].fillna('')
df['Decision_Taken_And_Reason'] = df['Decision_Taken_And_Reason'].fillna('')
df['Final_Decision'] = df['Final_Decision'].fillna('')

df.shape

df.isnull().sum()

# Presence of provisional decision content is found out on the complaint summary section
# Therefore those are scraped out of the dataframe and the content is placed into another feature column.
# Performing modeling on the input data which encompasses information on what the decision is about and the ombudsman findings is not ideal
# Thus, we process and scrape the content..

def extract_provisional_decision(text):
    keyword = "provisional decision"
    if keyword in text.lower():
        split_text = re.split(keyword, text, 1,flags=re.IGNORECASE)
        return split_text[0].strip(), split_text[1].strip() if len(split_text) > 1 else ""
    return text, ""

# Apply the function to the DataFrame
df[['Complaint_Explanation', 'provisional_decision']] = df['Complaint_Explanation'].apply(lambda x: pd.Series(extract_provisional_decision(x)))

df[df['provisional_decision'] != ''].shape

df[pd.isna(df['Decision_Taken_And_Reason']) & pd.isna(df['Final_Decision'])].shape

df[['date']]

df['date'] = pd.to_datetime(df['date'], format='%d %b %Y', errors='coerce')

df[['date']]

df.info()

"""## TEXT PREPROCESSING"""

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Lemmatization, removing punctuation marks, stopwords, special characters are performed below
# as a part of text preprocessing..

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
additional_stopwords = {'mr', 'mrs','Mr','Mrs'}

def clean_and_preprocess_text(text):
    # Convert to lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Remove numbers
    text = re.sub(r'\d+', '', text)
    # Tokenize text
    tokens = word_tokenize(text)
    # Remove stopwords and lemmatize tokens
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and word not in additional_stopwords]
    # Remove special characters and extra whitespace
    tokens = [re.sub(r'\W+', '', word) for word in tokens]
    tokens = [word for word in tokens if word.strip() != '']
    return ' '.join(tokens)

text_columns = ['Complaint_Info','Complaint_Explanation', 'Decision_Taken_And_Reason', 'Final_Decision']

new_df = df.copy()

# New dataframe is created from the existing dataframe where the processed text data goes into the new dataframe
# later stages, this is the dataframe which we are going to use everywhere...

for column in text_columns:
    new_df[column] = df[column].apply(lambda x: clean_and_preprocess_text(str(x)))

df['Complaint_Info'].iloc[4]

new_df['Complaint_Info'].iloc[4]

df['Complaint_Explanation'].iloc[6]

new_df['Complaint_Explanation'].iloc[6]

new_df.shape

complaint_columns = ['Complaint_Info','Complaint_Explanation']

new_df['complaint_data'] = new_df[complaint_columns].apply(lambda x: ' '.join(x), axis=1)

new_df['Complaint_Info'].iloc[0]

new_df['Complaint_Explanation'].iloc[0]

new_df['complaint_data'].iloc[0]

"""# ***EXPLORATORY DATA ANALYSIS (EDA)***

## ***ORGANIZATION ANALYSIS***
"""

# It is found out that SimplyBusiness operates on a different name called 'Xbridge Limited'
# Therefore identifying the number of records and printing them out...
# The number aggregates to over 36 records with 20 upheld cases and 16 not upheld cases...

new_df[new_df['company'] == 'Xbridge Limited'].shape

new_df.columns

new_df.company.nunique() # Printing out the number of unique companies present..

# Get the top 10 companies with the highest number of records
company_counts = new_df['company'].value_counts().head(10).reset_index()
company_counts.columns = ['company', 'count']

company_counts # Displaying the top 10 companies alongside their complaint count over the timeline..

# Bar plot indicating the companies alongside their complaint count.
# Corresponds to Figure 1 in the report..

plt.figure(figsize=(12,8), dpi = 200)
sns.barplot(x='count', y='company', data=company_counts, palette='viridis')
plt.title('Top 10 Companies', fontsize = 16)
plt.xlabel('Number of complaints', fontsize = 16)
plt.ylabel('Company Name', fontsize = 16)

plt.xticks(fontsize=14) # Describing the size of the X axis numeric values.
plt.yticks(fontsize=14) # Describing the size of the Y axis numeric values.
plt.show()

# Filter the dataframe for 'upheld' decisions
upheld_counts = new_df[new_df['decision'] == 'Upheld']['company'].value_counts().head(10).reset_index()
upheld_counts.columns = ['company', 'frequency']

# Filter the DataFrame for 'not upheld' decisions
not_upheld_counts = new_df[new_df['decision'] == 'Not upheld']['company'].value_counts().head(10).reset_index()
not_upheld_counts.columns = ['company', 'frequency']

# Barplot indicating the top 10 companies based on the number of upheld decisions.

plt.figure(figsize=(12, 8))
sns.barplot(x='frequency', y='company', data=upheld_counts, palette='viridis')
plt.title('Top 10 Companies by Upheld Decisions')
plt.xlabel('Frequency')
plt.ylabel('Company')
plt.show()

# Plot the top 10 companies with 'not upheld' decisions
plt.figure(figsize=(12, 8))
sns.barplot(x='frequency', y='company', data=not_upheld_counts, palette='magma')
plt.title('Top 10 Companies by Not Upheld Decisions')
plt.xlabel('Frequency')
plt.ylabel('Company')
plt.show()

new_df.decision.value_counts()

# Total claims per company
total_claims = new_df['company'].value_counts()

# Upheld claims per company
upheld_claims = new_df[new_df['decision'] == 'Upheld']['company'].value_counts()

# Calculate the ratio
ratio_df = (upheld_claims / total_claims).reset_index()
ratio_df.columns = ['company', 'ratio']

ratio_df.company.nunique()

ratio_df['ratio'] = ratio_df['ratio'].fillna(0)

#  Histogram plot depicting the distribution of companies on their upheld ratio..
# Corresponds to Figure 2 in the report..

import matplotlib.pyplot as plt
import seaborn as sns

# Create a histogram to show the distribution of ratio values
plt.figure(figsize=(12,9),dpi = 200)
sns.histplot(ratio_df['ratio'], bins=10, kde=True, stat="percent")
plt.title('Distribution of Upheld ratio',fontsize = 16)
plt.xlabel('Upheld ratio',fontsize = 16)
plt.ylabel('Percentage of Companies',fontsize = 16)

plt.xticks(fontsize=14) # Describing the size of the X axis numeric values.
plt.yticks(fontsize=14) # Describing the size of the Y axis numeric values.
plt.show()

ratio_df[(ratio_df['ratio'] >= 0.9) & (ratio_df['ratio'] <= 1.0)].shape

filtered_df = ratio_df[(ratio_df['ratio'] >= 0.0) & (ratio_df['ratio'] <= 0.1)]

# Calculate the percentage of companies in this range
percentage = (filtered_df.shape[0] / ratio_df.shape[0]) * 100

percentage

# Total claims per company
total_claims = new_df['company'].value_counts()

# Not Upheld claims per company
not_upheld_claims = new_df[new_df['decision'] == 'Not upheld']['company'].value_counts()

# Calculate the ratio
non_ratio_df = (not_upheld_claims / total_claims).reset_index()
non_ratio_df.columns = ['company', 'ratio']

non_ratio_df['ratio'] = non_ratio_df['ratio'].fillna(0)

# Distribution on the not upheld ratio across companies..


# Create a histogram to show the distribution of ratio values
plt.figure(figsize=(12,9), dpi = 200)
sns.histplot(non_ratio_df['ratio'], bins=10, kde=True, stat="percent")
plt.title('Distribution of Not upheld ratio',fontsize = 16)
plt.xlabel('Not upheld ratio',fontsize = 16)
plt.ylabel('Percentage of Companies',fontsize = 16)

plt.xticks(fontsize=14) # Describing the size of the X axis numeric values.
plt.yticks(fontsize=14) # Describing the size of the Y axis numeric values.
plt.show()

non_ratio_df[non_ratio_df['ratio'] == 1.0].shape

non_ratio_df[non_ratio_df['ratio'] == 0.0].shape

"""## ***TIMELINE ANALYSIS***"""

df.columns # Displaying columns of the dataframe present

new_df[['date']]

# Converting to datetime format and fetching the year, month and day accordingly..

new_df['year'] = new_df['date'].dt.year
new_df['month'] = new_df['date'].dt.month
new_df['day'] = new_df['date'].dt.day

new_df[['date','year','month','day']]

# Line plot depicting the aggregated complaints over the years present.
#  Corresponds to Figure 3 in the report.

complaints_per_year = new_df.groupby('year').size().reset_index(name='count')

plt.figure(figsize=(14,9), dpi = 200)
sns.lineplot(data=complaints_per_year, x='year', y='count', marker='o')
plt.title('Number of Complaints aggregated over the years',fontsize=16)
plt.xlabel('Year',fontsize=16)
plt.ylabel('Number of Complaints',fontsize=16)
plt.grid(True)
plt.xticks(complaints_per_year['year'],fontsize=14)  # Ensure all years are shown on the x-axis
plt.yticks(fontsize = 14)
plt.show()

# Lineplot depicting the aggregated complaints over the different months present.
# Correspnds to Figure 7 in the report.

complaints_per_month = new_df.groupby('month').size().reset_index(name='count')

plt.figure(figsize=(14,9), dpi=200)
sns.lineplot(data=complaints_per_month, x='month', y='count', marker='o')
plt.title('Number of Complaints aggregated over the months',fontsize=16)
plt.xlabel('Month of the year',fontsize=16)
plt.ylabel('Number of Complaints',fontsize=16)
plt.grid(True)

plt.xticks(complaints_per_month['month'],fontsize=14)  # Ensure all months are shown on the x-axis
plt.yticks(fontsize = 14)

plt.show()

# Lineplot depicting the aggregated complaints over the available days of the month..
complaints_per_day = new_df.groupby('day').size().reset_index(name='count')

plt.figure(figsize=(14,9), dpi=300)
sns.lineplot(data=complaints_per_day, x='day', y='count', marker='o')
plt.title('Number of complaints aggregated over the available days of the month',fontsize=16)
plt.xlabel('Day of the month',fontsize=16)
plt.ylabel('Number of Complaints',fontsize=16)
plt.grid(True)


plt.xticks(complaints_per_day['day'],fontsize=14)  # Ensure all months are shown on the x-axis
plt.yticks(fontsize = 14)
plt.show()

"""### ***UPHELD CLAIM ANALYSIS***"""

upheld_df = new_df[new_df['decision'] == 'Upheld']

upheld_df.shape

# Bar Plot indicatingthe yearly distribution using Seaborn package.
# Corresponds fo Figure 4 in the report.


plt.figure(figsize=(12, 8), dpi = 200)
sns.countplot(data=upheld_df, x='year', palette='viridis')
plt.title('Distribution of upheld complaints', fontsize = 16)
plt.xlabel('Year', fontsize = 16)
plt.ylabel('Number of complaints', fontsize = 16)


plt.xticks(fontsize=14) # Describing the size of the X axis numeric values.
plt.yticks(fontsize=14) # Describing the size of the Y axis numeric values.
plt.show()

# Plot the monthly distribution using Seaborn
plt.figure(figsize=(12, 8))
sns.countplot(data=upheld_df, x='month', palette='viridis')
plt.title('Monthly Distribution')
plt.xlabel('Month')
plt.ylabel('Count')
plt.show()

# Plot the daily distribution using Seaborn
plt.figure(figsize=(12, 8))
sns.countplot(data=upheld_df, x='day', palette='viridis')
plt.title('Daily Distribution')
plt.xlabel('Day')
plt.ylabel('Count')
plt.show()

"""#### CALCULATING THE RATIO OF UPHELD CLAIMS"""

# Aggregated over the Year

# Calculate the total number of claims per year
total_claims_per_year = new_df.groupby('year').size()
# Calculate the number of upheld claims per year
upheld_claims_per_year = upheld_df.groupby('year').size()

# Calculate the ratio of upheld claims to total claims per year
ratio_per_year_1 = (upheld_claims_per_year / total_claims_per_year).reset_index()
ratio_per_year_1.columns = ['year', 'ratio']
ratio_per_year_1 = ratio_per_year_1.fillna(0)  # Fill NaN values with 0 if there are years with no upheld claims

ratio_per_year_1

# Line Plot for the upheld ratio across years present.

plt.figure(figsize=(12, 8), dpi = 200)
sns.lineplot(data=ratio_per_year_1, x='year', y='ratio', marker='o')
plt.title('Distribution of Upheld ratio', fontsize = 16)
plt.xlabel('Year', fontsize = 16)
plt.ylabel('Ratio', fontsize = 16)

plt.xticks(ratio_per_year_1['year'], rotation=45, fontsize = 14)
plt.yticks(fontsize = 14)
plt.grid(True)
plt.show()

# Aggregated over the Month

total_claims_per_month = new_df.groupby('month').size()
upheld_claims_per_month = upheld_df.groupby('month').size()

ratio_per_month_1 = (upheld_claims_per_month / total_claims_per_month).reset_index()
ratio_per_month_1.columns = ['month','ratio']
ratio_per_month_1 = ratio_per_month_1.fillna(0)

ratio_per_month_1

# Plot the ratio across months
plt.figure(figsize=(12, 8))
sns.lineplot(data=ratio_per_month_1, x='month', y='ratio', marker='o')
plt.title('Ratio of Upheld Claims to Total Claims per Month')
plt.xlabel('Month')
plt.ylabel('Ratio of Upheld Claims')
plt.xticks(ratio_per_month_1['month'], rotation=45)
plt.grid(True)
plt.show()

"""### ***NOT UPHELD DECISION ANALYSIS***"""

not_upheld_df = new_df[new_df['decision'] == 'Not upheld']

not_upheld_df.shape

# Bar Plot indicating the yearly distribution using Seaborn package.
# Corresponds fo Figure 5 in the report.


plt.figure(figsize=(12, 8), dpi = 200)
sns.countplot(data=not_upheld_df, x='year', palette='viridis')
plt.title('Yearly Distribution', fontsize = 16)
plt.xlabel('Year', fontsize = 16)
plt.ylabel('Count', fontsize = 16)

plt.xticks(fontsize=14) # Describing the size of the X axis numeric values.
plt.yticks(fontsize=14) # Describing the size of the Y axis numeric values.
plt.show()

# Plot the monthly distribution using Seaborn
plt.figure(figsize=(12, 8))
sns.countplot(data=not_upheld_df, x='month', palette='viridis')
plt.title('Monthly Distribution')
plt.xlabel('Month')
plt.ylabel('Count')
plt.show()

# Plot the daily distribution using Seaborn
plt.figure(figsize=(12, 8))
sns.countplot(data=not_upheld_df, x='day', palette='viridis')
plt.title('Daily Distribution')
plt.xlabel('Day')
plt.ylabel('Count')
plt.show()

"""#### CALCULATING THE RATIO"""

# Aggregated over the Year

# Calculate the total number of claims per year
total_claims_per_year = new_df.groupby('year').size()

# Calculate the number of upheld claims per year
not_upheld_claims_per_year = not_upheld_df.groupby('year').size()

# Calculate the ratio of upheld claims to total claims per year
ratio_per_year_2 = (not_upheld_claims_per_year / total_claims_per_year).reset_index()
ratio_per_year_2.columns = ['year', 'ratio']
ratio_per_year_2 = ratio_per_year_2.fillna(0)  # Fill NaN values with 0 if there are years with no upheld claims

ratio_per_year_2

# Plot the ratio across years
plt.figure(figsize=(12, 8))
sns.lineplot(data=ratio_per_year_2, x='year', y='ratio', marker='o')
plt.title('Ratio of Not upheld Claims to Total Claims per Year')
plt.xlabel('Year')
plt.ylabel('Ratio of Not Upheld Claims')
plt.xticks(ratio_per_year_2['year'], rotation=0)
plt.grid(True)
plt.show()

# Aggregated over the Month

# Calculate the number of upheld claims per year
not_upheld_claims_per_month = not_upheld_df.groupby('month').size()

# Calculate the ratio of upheld claims to total claims per year
ratio_per_month_2 = (not_upheld_claims_per_month / total_claims_per_month).reset_index()
ratio_per_month_2.columns = ['month', 'ratio']
ratio_per_month_2 = ratio_per_month_2.fillna(0)  # Fill NaN values with 0 if there are years with no upheld claims

ratio_per_month_2

# Plot the ratio across years
plt.figure(figsize=(12, 8))
sns.lineplot(data=ratio_per_month_2, x='month', y='ratio', marker='o')
plt.title('Ratio of Not upheld Claims to Total Claims per Month')
plt.xlabel('Month')
plt.ylabel('Ratio of Not Upheld Claims')
plt.xticks(ratio_per_month_2['month'], rotation=0)
plt.grid(True)
plt.show()

# Two lineplots on a single workspace depicting upheld and not upheld ratio respectively..
# Corresponding to Figure 6 in the report

fig, ax = plt.subplots(figsize=(16, 9), dpi=200)

# Plotting upheld ratio
ax.plot(ratio_per_year_1['year'], ratio_per_year_1['ratio'], marker='o', linestyle='-', color='b', label='Upheld ratio')

# Plotting not upheld ratio
ax.plot(ratio_per_year_2['year'], ratio_per_year_2['ratio'], marker='s', linestyle='-', color='r', label='Not upheld ratio')

# Labels and title
ax.set_xlabel('Year', fontsize=16)
ax.set_ylabel('Ratio', fontsize=16)
ax.set_title('Distribution of Upheld and Not upheld ratio on a yearly basis', fontsize=16)


ax.set_xticks(ratio_per_year_1['year'])
# Tick parameters
ax.tick_params(axis='x', labelsize=14)
ax.tick_params(axis='y', labelsize=14)
# Grid
ax.grid(True)


# Legend
ax.legend(loc='upper right', fontsize=14)
# Show plot
plt.show()

# Corresponds to Figure 8 in the report..

# Plotting two line plots of upheld and not upheld ratio aggregated over the months present in a single workspace.

fig, ax = plt.subplots(figsize=(16, 9), dpi=200)

# Plotting upheld ratio
ax.plot(ratio_per_month_1['month'], ratio_per_month_1['ratio'], marker='o', linestyle='-', color='b', label='Upheld ratio')

# Plotting not upheld ratio
ax.plot(ratio_per_month_2['month'], ratio_per_month_2['ratio'], marker='s', linestyle='-', color='r', label='Not upheld ratio')

# Labels and title
ax.set_xlabel('Month', fontsize=16)
ax.set_ylabel('Ratio', fontsize=16)
ax.set_title('Distribution of Upheld and Not upheld ratio on a monthly basis', fontsize=16)

ax.set_xticks(ratio_per_month_1['month'])
# Tick parameters
ax.tick_params(axis='x', labelsize=14)
ax.tick_params(axis='y', labelsize=14)
# Grid
ax.grid(True)


# Legend
ax.legend(loc='upper right', fontsize=14)
# Show plot
plt.show()

"""## ***TEXT DATA ANALYSIS***

### ***WORD AND CHARACTER COUNT***
"""

# Define the text_statistics function
def text_statistics(text):
    # Remove punctuation and convert text to lowercase
    cleaned_text = re.sub(r'[^\w\s]', '', text.lower())

    # Tokenize text into words
    words = cleaned_text.split()

    # Total word count
    total_word_count = len(words)

    # Unique word count
    unique_word_count = len(set(words))

    # Frequency distribution of words
    word_frequencies = Counter(words)

    # Most common words
    most_common_words = word_frequencies.most_common(10)

    # Average word length
    avg_word_length = sum(len(word) for word in words) / total_word_count


    # Compile statistics into a dictionary
    stats = {
        'Total Word Count': total_word_count,
        'Unique Word Count': unique_word_count,
        'Most Common Words': most_common_words,
        'Average Word Length': avg_word_length
    }

    return stats

# Emulating the distribution histogram plot on the word length present over every complaint record present..
# Corresponding to Figure 9 in the report

new_df['word_length_complaint'] = new_df['complaint_data'].apply(lambda x: len(str(x).split()))

# Distribution of word lengths
plt.figure(figsize=(12,9), dpi = 200)
sns.histplot(new_df['word_length_complaint'], bins=100, kde=True)
plt.title('Distribution of word length in encompassed complaint information',fontsize = 16)
plt.xlabel('Word Length', fontsize = 16)
plt.ylabel('Frequency',fontsize = 16)


plt.xticks(rotation=0, fontsize = 14)
plt.yticks(fontsize = 14)
plt.grid(True)
plt.show()

# Concatenate all text from the 'combined_data' column
all_text = ' '.join(new_df['complaint_data'])

# Get text statistics for the concatenated text
stats = text_statistics(all_text)

# Print the results
print(f"Total Word Count: {stats['Total Word Count']}")
print(f"Unique Word Count: {stats['Unique Word Count']}")
print(f"Most Common Words: {stats['Most Common Words']}")
print(f"Average Word Length: {stats['Average Word Length']:.2f}")

# Ploting the top 10 common words
#Corresponding to Figure 10 in the report..


# Separate words and their corresponding scores
words, scores = zip(*stats['Most Common Words'])

# Plotting
plt.figure(figsize=(14,9), dpi = 200)
plt.barh(words, scores, color='maroon')
plt.xlabel('Count',fontsize = 16)
plt.ylabel('Words',fontsize = 16)
plt.title('Top 10 common words in the complaint section', fontsize = 16)
plt.gca().invert_yaxis()  # Invert y-axis to have the highest scores at the top


plt.xticks(fontsize=14) # Setting the size of the X axis values.
plt.yticks(fontsize=14) # Setting the size of the Y axis values.
plt.tight_layout()  # Adjust layout to prevent clipping
plt.show()

"""### ***WORDCLOUD ANALYSIS***"""

# User defined function to generate the wordcloud for the text data present in the dataframe.

def generate_word_cloud(corpus_data: str, cmap: str, title: str):

    wordcloud = WordCloud(background_color = 'black', width = 800, height = 400,
                          colormap = cmap, max_words = 156, contour_width = 3,
                          max_font_size = 80, contour_color = 'blue',
                          random_state = 0)

    wordcloud.generate(corpus_data)

    plt.figure(figsize=(12,8))
    plt.title(f'Word Cloud for {title}', fontsize=10)
    plt.imshow(wordcloud, interpolation = 'bilinear')
    plt.axis("off")
    plt.figure()

text = ' '.join(new_df['Complaint_Info'].dropna().values)
generate_word_cloud(text, cmap = 'viridis',title = 'Complaint_Information')

text = ' '.join(new_df['Complaint_Explanation'].dropna().values)
generate_word_cloud(text, cmap = 'tab20b', title = 'Complaint_Explanation')

# Wordcloud is generated for the encompassed complaint information (Input data)...
## Corresponds to Figure 11 in the report..

text = ' '.join(new_df['complaint_data'].dropna().values)
generate_word_cloud(text, cmap = 'hsv', title = 'Encompassed Complaint Information (Input Data)')

"""### ***FETCHING THE TOP 10 WORDS***"""

# Compute TF-IDF scores for each category
vectorizer = TfidfVectorizer(max_features=1000)

upheld_tfidf_matrix = vectorizer.fit_transform(upheld_df['complaint_data'])

# Extract the TF-IDF feature names
feature_names = vectorizer.get_feature_names_out()

# Compute mean TF-IDF scores for each decision category
upheld_tfidf_mean = upheld_tfidf_matrix.mean(axis=0).A1

# Get the top 10 words for each category
top_15_upheld_indices = upheld_tfidf_mean.argsort()[-15:][::-1]
top_15_upheld_words = [(feature_names[i], upheld_tfidf_mean[i]) for i in top_15_upheld_indices]

print("Top 10 words contributing to Upheld decisions:\n")
for word, score in top_15_upheld_words:
    print(f"{word}: {score}")

# Emulating the barplot which depicts the top 15 words contributing to the upheld decisions.
## Corresponds to Figure 12 in the report.

import matplotlib.pyplot as plt

# Separate words and their corresponding scores
words, scores = zip(*top_15_upheld_words)

# Plotting
plt.figure(figsize=(14,9), dpi = 200)
plt.barh(words, scores, color='darkblue')
plt.xlabel('TF-IDF Score',fontsize = 16)
plt.ylabel('Words',fontsize = 16)
plt.title('Top 15 Words Contributing to Upheld decisions', fontsize = 16)
plt.gca().invert_yaxis()  # Invert y-axis to have the highest scores at the top


plt.xticks(fontsize=14) # Setting the size of the X axis values.
plt.yticks(fontsize=14) # Setting the size of the Y axis values.
plt.tight_layout()  # Adjust layout to prevent clipping
plt.show()

not_upheld_tfidf_matrix = vectorizer.fit_transform(not_upheld_df['complaint_data'])
not_upheld_tfidf_mean = not_upheld_tfidf_matrix.mean(axis=0).A1

top_15_not_upheld_indices = not_upheld_tfidf_mean.argsort()[-15:][::-1]
top_15_not_upheld_words = [(feature_names[i], not_upheld_tfidf_mean[i]) for i in top_15_not_upheld_indices]

print("\nTop 15 words contributing to Not Upheld decisions:\n")
for word, score in top_15_not_upheld_words:
    print(f"{word}: {score}")

# Barplot for indicating the top 15 words which are more prominent in the Not upheld decisions..
# Corresponds to Figure 13 in the report.

import matplotlib.pyplot as plt



# Separate words and their corresponding scores
words, scores = zip(*top_15_not_upheld_words)

# Plotting
plt.figure(figsize=(14,9), dpi = 200)
plt.barh(words, scores, color='maroon')
plt.xlabel('TF-IDF Score',fontsize = 16)
plt.ylabel('Words',fontsize = 16)
plt.title('Top 15 Words Contributing to Not upheld decisions', fontsize = 16)
plt.gca().invert_yaxis()  # Invert y-axis to have the highest scores at the top


plt.xticks(fontsize=14) # Setting the size of the X axis values.
plt.yticks(fontsize=14) # Setting the size of the Y axis values.
plt.tight_layout()  # Adjust layout to prevent clipping
plt.show()

"""### ***SENTIMENT ANALYSIS***"""

from textblob import TextBlob

# Sentiment Analysis
new_df['sentiment'] = new_df['complaint_data'].apply(lambda x: TextBlob(x).sentiment.polarity)

new_df[['sentiment']]

new_df.sentiment.value_counts()

new_df[new_df['sentiment']>0].shape , new_df[new_df['decision'] == 'Upheld'].shape

new_df[new_df['sentiment']<0].shape , new_df[new_df['decision'] == 'Not upheld'].shape

plt.figure(figsize=(50,30))
plt.margins(0.02)
plt.xlabel('Sentiment', fontsize=50)
plt.xticks(fontsize=40)
plt.ylabel('Frequency', fontsize=50)
plt.yticks(fontsize=40)
plt.hist(new_df['sentiment'], bins=50)
plt.title('Sentiment Distribution', fontsize=60)
plt.show()

"""## ***CATEGORY EVALUATION (TOPIC MODELLING) ----- LDA IMPLEMENTATION***"""

# Since LDA model is a bit stochastic in nature, the plots do tend to alter since the number of topics change sometimes..
# Figures 14 and 15 are taken from this section after fitting the LDA model..

# The process of implementation is as follows :
# A dictionary is created with the corpus of data given , in our case it is the complaint data.
# Then a series of topics are generated within a range specified..
# For every number, a coherence score is calculated which is evaluated amongst other coherence scores computed
# The corresponding number with the best coherence score is taken out..
# The top 15 words are printed out on every topic , where on manually observing , we create a dictionary with appropriate label names..
# Create a column in the dataframe and assign the dominant topic's label name
# We therefore get the appropriate category after which we carry on further analysis.

# Prepare data for LDA
texts = new_df['complaint_data'].apply(lambda x: x.lower().split()).tolist()
dictionary = Dictionary(texts)

# Filter out extremes to limit the number of features
dictionary.filter_extremes(no_below=1, no_above=0.85, keep_n=5000)

corpus = [dictionary.doc2bow(text) for text in texts]

# List of topic numbers to try
topic_nums = list(np.arange(7, 10, 1))

# Store coherence scores
coherence_scores = []

coherence_type = 'c_v'

for num in topic_nums:
    print(num)
    lda = LdaModel(
        corpus=corpus,
        num_topics=num,
        id2word=dictionary,
        chunksize=2000,
        passes=5,
        alpha='auto',
        random_state=42
    )

    # Run the coherence model to get the score
    cm = CoherenceModel(
        model=lda,
        texts=texts,
        dictionary=dictionary,
        coherence=coherence_type
    )

    coherence_score = round(cm.get_coherence(), 5)
    coherence_scores.append((num, coherence_score))
    print(f"Coherence Score for {num} topics: {coherence_score}")

# Display the coherence scores
print("Coherence Scores for different numbers of topics:")
for num, score in coherence_scores:
    print(f"{num} topics: {score}")

# Find the best number of topics based on the highest coherence score
optim_num_topics = sorted(coherence_scores, key=lambda x: x[1], reverse=True)[0][0]

print(f"Best number of topics: {optim_num_topics}")

# Fit LDA model with the optimal topics which had the best coherence score..
lda_model = LdaModel(
    corpus=corpus,
    num_topics=optim_num_topics,
    id2word=dictionary,
    chunksize=2000,
    passes=5,
    alpha='auto',
    random_state=42
)

# Display the topics
for i, topic in lda_model.show_topics(formatted=False, num_words=15):
    print(f"Topic {i}: {[word for word, _ in topic]}")

'''
# Map topics to labels
topic_labels = {
    0: 'Legal / Business Insurance',
    1: 'Mortgage PPI',
    2: 'Loan repayment PPI',
    3: 'Medical Insurance',
    4: 'Others',
    5: 'Home Insurance',
    6: 'Vehicle Insurance',
    7: 'Credit Card PPI'
}

'''


topic_labels = {
    0: 'Legal / Business Insurance',
    1: 'Mortgage PPI',
    2: 'Loan repayment PPI',
    3: 'Medical Insurance',
    4: 'Credit Card PPI',
    5: 'Home Insurance',
    6: 'Vehicle Insurance'
}

# Assign the dominant topic to each document
topics = lda_model[corpus]
dominant_topics = [max(t, key=lambda x: x[1])[0] for t in topics]

new_df['Topic_Cluster'] = dominant_topics

# Create a new column in the DataFrame with the corresponding topic labels
new_df['complaint_category'] = new_df['Topic_Cluster'].map(topic_labels)

new_df.complaint_category.value_counts()

# Countplot using Seaborn
plt.figure(figsize=(12,8))
sns.countplot(data=new_df, x='complaint_category', order=new_df['complaint_category'].value_counts().index,palette='viridis')
plt.title('Count of Complaints by Category')
plt.xlabel('Complaint category')
plt.ylabel('Count')
plt.xticks(rotation=30)
plt.show()

upheld_df = new_df[new_df['decision'] == 'Upheld']
not_upheld_df = new_df[new_df['decision'] == 'Not upheld']

upheld_df.shape , not_upheld_df.shape

# Calculate the total number of claims per category present
total_claims_per_category = new_df.groupby('complaint_category').size()

# Calculate the number of upheld claims per category
upheld_claims_per_category = upheld_df.groupby('complaint_category').size()


# Calculate the ratio of upheld claims to total claims per year
upheld_ratio = (upheld_claims_per_category / total_claims_per_category).reset_index()
upheld_ratio.columns = ['category', 'ratio']
upheld_ratio = upheld_ratio.fillna(0)  # Fill NaN values with 0 if there are years with no upheld claims

new_df[(new_df['complaint_category'] == 'Home Insurance') & (new_df['decision'] == 'Upheld')].shape

upheld_claims_per_category , total_claims_per_category

upheld_ratio

# Plot the ratio across years
plt.figure(figsize=(12, 8))
sns.lineplot(data=upheld_ratio, x='category', y='ratio', marker='o')
plt.title('Ratio of upheld Claims to Total Claims per every available category present')
plt.xlabel('Category of the complaint')
plt.ylabel('Ratio of Upheld Claims')
plt.xticks(upheld_ratio['category'], rotation = 30)
plt.grid(True)
plt.show()

# Calculate the total number of claims per category present
total_claims_per_category = new_df.groupby('complaint_category').size()

# Calculate the number of upheld claims per category
not_upheld_claims_per_category = not_upheld_df.groupby('complaint_category').size()


# Calculate the ratio of upheld claims to total claims per year
not_upheld_ratio = (not_upheld_claims_per_category / total_claims_per_category).reset_index()
not_upheld_ratio.columns = ['category', 'ratio']
not_upheld_ratio = not_upheld_ratio.fillna(0)  # Fill NaN values with 0 if there are years with no upheld claims

not_upheld_claims_per_category

not_upheld_ratio

# Plot the ratio across years
plt.figure(figsize=(12, 8))
sns.lineplot(data=not_upheld_ratio, x='category', y='ratio', marker='o')
plt.title('Ratio of Not upheld Claims to Total Claims per every available category present')
plt.xlabel('Category of the complaint')
plt.ylabel('Ratio of Not Upheld Claims')
plt.xticks(not_upheld_ratio['category'], rotation = 30)
plt.grid(True)
plt.show()

# Plot the ratios
plt.figure(figsize=(14, 8))

sns.lineplot(data=upheld_ratio, x='category', y='ratio', marker='o', label='Upheld Ratio', color='b')
sns.lineplot(data=not_upheld_ratio, x='category', y='ratio', marker='o', label='Not Upheld Ratio', color='r')

plt.title('Ratios of Upheld and Not Upheld Claims per Complaint Category')
plt.xlabel('Complaint Category')
plt.ylabel('Ratio')
plt.xticks(rotation=30)
plt.legend()
plt.grid(True)
plt.show()

"""## ***INSIGHTS ON THE OMBUDSMAN DECISION MAKING***"""

# We will be taking the ombudsman decisions into consideration (extracted third and fourth sections are into consideration)...

new_df.columns

new_df.isnull().sum()

from wordcloud import WordCloud
import matplotlib.pyplot as plt, seaborn as sb

def generate_word_cloud(corpus_data: str, cmap: str, title: str):

    wordcloud = WordCloud(background_color = 'black', width = 800, height = 400,
                          colormap = cmap, max_words = 156, contour_width = 3,
                          max_font_size = 80, contour_color = 'blue',
                          random_state = 0)

    wordcloud.generate(corpus_data)

    plt.figure(figsize=(12,8))
    plt.title(f'Word Cloud for {title}', fontsize=10)
    plt.imshow(wordcloud, interpolation = 'bilinear')
    plt.axis("off")
    plt.figure()

## Corresponds to Figure 16 in the report..
# Wordcloud generated for rthe ombudsman findings (Section 3 from the decision file).

text = ' '.join(new_df['Decision_Taken_And_Reason'].dropna().values)
generate_word_cloud(text, cmap = 'Accent', title = 'Ombudsman Analysis')

## wordcoloud for the final decision undertaken by the ombudsman.

text = ' '.join(new_df['Final_Decision'].dropna().values)
generate_word_cloud(text, cmap = 'tab20b', title = 'Final Decision')

from sklearn.feature_extraction.text import CountVectorizer

def get_top_n_ngrams(corpus, n=2, num_ngrams=10):
    vectorizer = CountVectorizer(ngram_range=(n, n))
    ngrams = vectorizer.fit_transform(corpus)
    ngrams_freq = ngrams.sum(axis=0).A1
    ngrams_freq = sorted([(freq, ngram) for ngram, freq in zip(vectorizer.get_feature_names_out(), ngrams_freq)], reverse=True)
    return ngrams_freq[:num_ngrams]

bigrams = get_top_n_ngrams(new_df['Decision_Taken_And_Reason'].dropna().values, n=2)
trigrams = get_top_n_ngrams(new_df['Decision_Taken_And_Reason'].dropna().values, n=3)

print("Top Bigrams:", bigrams)

print("Top Trigrams:", trigrams)

new_df['word_length'] = new_df['Decision_Taken_And_Reason'].apply(lambda x: len(str(x).split()))

# Distribution of word lengths
plt.figure(figsize=(12, 6))
sns.histplot(new_df['word_length'], bins=20, kde=True)
plt.title('Distribution of Word Length in Ombudsman Reasoning')
plt.xlabel('Word Length')
plt.ylabel('Frequency')
plt.show()

new_df['word_length_final_decision'] = new_df['Final_Decision'].apply(lambda x: len(str(x).split()))

# Distribution of word lengths
plt.figure(figsize=(12, 6))
sns.histplot(new_df['word_length_final_decision'], bins=30, kde=True)
plt.title('Distribution of Word Length in Ombudsman Decision')
plt.xlabel('Word Length')
plt.ylabel('Frequency')
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
# Compute TF-IDF scores for each category
vectorizer = TfidfVectorizer(max_features=1000)

omb_matrix = vectorizer.fit_transform(new_df['Decision_Taken_And_Reason'])

# Extract the TF-IDF feature names
feature_names = vectorizer.get_feature_names_out()

# Compute mean TF-IDF scores for each decision category
omb_matrix_mean = omb_matrix.mean(axis=0).A1

# Get the top 10 words for each category
top_15_omb_indices = omb_matrix_mean.argsort()[-15:][::-1]
top_15_omb_words = [(feature_names[i], omb_matrix_mean[i]) for i in top_15_omb_indices]

print("Top 15 words used in Ombudsman analysis:\n")
for word, score in top_15_omb_words:
    print(f"{word}: {score}")

# Bar plot indicating top 15 words which are more frequent in ombudsman findings..


# Separate words and their corresponding scores
words, scores = zip(*top_15_omb_words)

# Plotting
plt.figure(figsize=(14,9), dpi = 200)
plt.barh(words, scores, color='midnightblue')
plt.xlabel('TF-IDF Score',fontsize = 16)
plt.ylabel('Words',fontsize = 16)
plt.title('Top 15 Words which are more prominent on ombudsman analysis', fontsize = 16)
plt.gca().invert_yaxis()  # Invert y-axis to have the highest scores at the top


plt.xticks(fontsize=14) # Setting the size of the X axis values.
plt.yticks(fontsize=14) # Setting the size of the Y axis values.
plt.tight_layout()  # Adjust layout to prevent clipping
plt.show()

from sklearn.feature_extraction.text import TfidfVectorizer
# Compute TF-IDF scores for each category
vectorizer = TfidfVectorizer(max_features=1000)

omb_final_mat = vectorizer.fit_transform(new_df['Final_Decision'])

# Extract the TF-IDF feature names
feature_names = vectorizer.get_feature_names_out()

# Compute mean TF-IDF scores for each decision category
omb_final_mat_mean = omb_final_mat.mean(axis=0).A1

# Get the top 10 words for each category
top_15_omb_final_indices = omb_final_mat_mean.argsort()[-15:][::-1]
top_15_omb_final_words = [(feature_names[i], omb_final_mat_mean[i]) for i in top_15_omb_final_indices]

print("Top 15 words used in Ombudsman analysis:\n")
for word, score in top_15_omb_final_words:
    print(f"{word}: {score}")

# Separate words and their corresponding scores
words, scores = zip(*top_15_omb_final_words)

# Plotting
plt.figure(figsize=(14,9), dpi = 200)
plt.barh(words, scores, color='maroon')
plt.xlabel('TF-IDF Score',fontsize = 16)
plt.ylabel('Words',fontsize = 16)
plt.title('Top 15 Words which are more prominent on the decisions', fontsize = 16)
plt.gca().invert_yaxis()  # Invert y-axis to have the highest scores at the top


plt.xticks(fontsize=14) # Setting the size of the X axis values.
plt.yticks(fontsize=14) # Setting the size of the Y axis values.
plt.tight_layout()  # Adjust layout to prevent clipping
plt.show()

"""# ***FEATURE EXTRACTION TECHNIQUES***

## ***TFIDF METHOD***
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=1000)
X = vectorizer.fit_transform(new_df['complaint_data'])
y = new_df['decision'].apply(lambda x: 1 if x == 'Upheld' else 0)

# Convert TF-IDF matrix to DataFrame
tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())
print(tfidf_df.head())

"""## ***WORD2VEC METHOD***"""

import gensim
from gensim.models import Word2Vec
import numpy as np

# Tokenize the text data
tokenized_text = [text.split() for text in new_df['complaint_data']]

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1, workers=4)

# Function to get Word2Vec vectors
def get_word2vec_vectors(text, model, vector_size=100):
    words = text.split()
    vector = np.zeros(vector_size)
    count = 0
    for word in words:
        if word in model.wv:
            vector += model.wv[word]
            count += 1
    if count != 0:
        vector /= count
    return vector

# Create X and y for Word2Vec
X = np.array([get_word2vec_vectors(text, word2vec_model) for text in new_df['complaint_data']])
y = new_df['decision'].apply(lambda x: 1 if x == 'Upheld' else 0)

"""## ***GLOVE METHOD***"""

import numpy as np

# Load GloVe vectors from the file present in the already det directory.
glove_path = 'glove.6B.100d.txt'
embeddings_index = {}
with open(glove_path, 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs

# Function to get GloVe vectors
def get_glove_vectors(text, embeddings_index, vector_size=100):
    words = text.split()
    vector = np.zeros(vector_size)
    count = 0
    for word in words:
        if word in embeddings_index:
            vector += embeddings_index[word]
            count += 1
    if count != 0:
        vector /= count
    return vector

# Create X and y for GloVe
X = np.array([get_glove_vectors(text, embeddings_index) for text in new_df['complaint_data']])
y = new_df['decision'].apply(lambda x: 1 if x == 'Upheld' else 0)

"""# ***MODELLING***"""

# Performing the 80-20 train test split
# 80% of records go into the train dataset where the remaining 20% of records into the test dataset.

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.shape, X_test.shape

"""## ***LOGISTC REGRESSION MODEL***"""

# Fitting the Logistic Regression model.

model = LogisticRegression()
model.fit(X_train, y_train)

# Predictions on the test dataframe

y_logistic_pred = model.predict(X_test)

# Fetching the accuracy score.

accuracy_test = accuracy_score(y_test,y_logistic_pred)

# Fetching the precision, recall and F1- scores on the test dataset

precision_test = precision_score(y_test,y_logistic_pred)
recall_test = recall_score(y_test,y_logistic_pred)
f1_test = f1_score(y_test,y_logistic_pred)

# AUC-ROC Score
auc_roc = roc_auc_score(y_test, y_logistic_pred)

print(f'Test Accuracy: {accuracy_test}')

print(f'Test Precision: {precision_test}')
print(f'Test Recall: {recall_test}')
print(f'Test F1 Score: {f1_test}')

print(f"AUC-ROC Score: {auc_roc}")

y_test.value_counts()

# Printing out the confusion matrix for the Logistic Regression model evaluated on the test dataset.

import matplotlib.pyplot as plt
cm = confusion_matrix(y_test, y_logistic_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Upheld', 'Not Upheld'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Logistic Regression Model')
plt.show()

print("Classification Report:\n", classification_report(y_test, y_logistic_pred))

"""## ***RANDOM FOREST MODEL***"""

# Train a Random Forest model
random_forest_model = RandomForestClassifier(random_state=42, n_estimators=500)
random_forest_model.fit(X_train, y_train)

# Predict on the test set
y_pred_rand_model = random_forest_model.predict(X_test)

accuracy_rand_test = accuracy_score(y_test,y_pred_rand_model)

# Fetching the precision, recall and F1- scores on the test dataset

precision_rand_test = precision_score(y_test,y_pred_rand_model)
recall_rand_test = recall_score(y_test,y_pred_rand_model)
f1_rand_test = f1_score(y_test,y_pred_rand_model)

# AUC-ROC Score
auc_roc_rand_test = roc_auc_score(y_test, y_pred_rand_model)

print(f'Test Accuracy: {accuracy_rand_test}')

print(f'Test Precision: {precision_rand_test}')
print(f'Test Recall: {recall_rand_test}')
print(f'Test F1 Score: {f1_rand_test}')

print(f"AUC-ROC Score: {auc_roc_rand_test}")

# Printing out the confusion matrix for the Random Forest model evaluated on the test dataset.

cm = confusion_matrix(y_test, y_pred_rand_model)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Upheld', 'Not Upheld'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for Random Forrest Classification Model')
plt.show()

"""## ***BERT MODEL***"""

# Preparing the raw data corpus encompassing the complaint information.
# Passing the raw ( non preprocessed) text is essential for BERT to capture the bidirectional dependencies effectively..

complaint_columns = ['complaint_info','complaint_explanation']
df['complaint_data'] = df[complaint_columns].apply(lambda x: ' '.join(x), axis=1)

# Label encoding
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['decision'])

# Split the data
X_train, X_test, y_train, y_test = train_test_split(df['complaint_data'], df['label'], test_size=0.2, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

# Tokenizer and Model
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)

# Custom Function for performing BERT tokenization.

class CustomDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]

        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            return_token_type_ids=False,
            pad_to_max_length=True,
            return_attention_mask=True,
            return_tensors='pt',
        )

        return {
            'text': text,
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }

def create_data_loader(df, tokenizer, max_len, batch_size):
    ds = CustomDataset(
        texts=df['complaint_data'].to_numpy(),
        labels=df['label'].to_numpy(),
        tokenizer=tokenizer,
        max_len=max_len
    )

    return DataLoader(ds, batch_size=batch_size, num_workers=4)

# Generating data loaders on the train, val and test dataset.
BATCH_SIZE = 16
MAX_LEN = 128

train_data_loader = create_data_loader(pd.DataFrame({'complaint_data': X_train, 'label': y_train}), tokenizer, MAX_LEN, BATCH_SIZE)
val_data_loader = create_data_loader(pd.DataFrame({'complaint_data': X_val, 'label': y_val}), tokenizer, MAX_LEN, BATCH_SIZE)
test_data_loader = create_data_loader(pd.DataFrame({'complaint_data': X_test, 'label': y_test}), tokenizer, MAX_LEN, BATCH_SIZE)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

EPOCHS = 1
optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
loss_fn = torch.nn.CrossEntropyLoss().to(device)

def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):
    model = model.train()
    losses = []
    correct_predictions = 0

    for d in data_loader:
        input_ids = d['input_ids'].to(device)
        attention_mask = d['attention_mask'].to(device)
        labels = d['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        _, preds = torch.max(outputs.logits, dim=1)
        loss = loss_fn(outputs.logits, labels)

        correct_predictions += torch.sum(preds == labels)
        losses.append(loss.item())

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    return correct_predictions.double() / n_examples, np.mean(losses)

def eval_model(model, data_loader, loss_fn, device, n_examples):
    model = model.eval()
    losses = []
    correct_predictions = 0

    with torch.no_grad():
        for d in data_loader:
            input_ids = d['input_ids'].to(device)
            attention_mask = d['attention_mask'].to(device)
            labels = d['label'].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            _, preds = torch.max(outputs.logits, dim=1)
            loss = loss_fn(outputs.logits, labels)

            correct_predictions += torch.sum(preds == labels)
            losses.append(loss.item())

    return correct_predictions.double() / n_examples, np.mean(losses)

# Training the model

for epoch in range(EPOCHS):
    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
        model,
        train_data_loader,
        loss_fn,
        optimizer,
        device,
        None,
        len(X_train)
    )

    print(f'Train loss {train_loss} accuracy {train_acc}')

    val_acc, val_loss = eval_model(
        model,
        val_data_loader,
        loss_fn,
        device,
        len(X_val)
    )

    print(f'Val loss {val_loss} accuracy {val_acc}')

# Evaluate on the test set
test_acc, test_loss = eval_model(
    model,
    test_data_loader,
    loss_fn,
    device,
    len(X_test)
)
print(f'Test loss {test_loss} accuracy {test_acc}')

# Predict on the test set
model = model.eval()
y_pred_bert_torch = []
y_true_bert_torch = []

with torch.no_grad():
    for d in test_data_loader:
        input_ids = d['input_ids'].to(device)
        attention_mask = d['attention_mask'].to(device)
        labels = d['label'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
        _, preds = torch.max(outputs.logits, dim=1)

        y_pred_bert_torch.extend(preds)
        y_true_bert_torch.extend(labels)

# Convert to numpy arrays
y_pred_bert_torch = torch.stack(y_pred_bert_torch).cpu().numpy()
y_true_bert_torch = torch.stack(y_true_bert_torch).cpu().numpy()

# Fetching the precision, recall and F1 scores on the test dataset.

precision_test_bert = precision_score(y_pred_bert_torch,y_true_bert_torch)
recall_test_bert = recall_score(y_pred_bert_torch,y_true_bert_torch)
f1_test_bert = f1_score(y_pred_bert_torch,y_true_bert_torch)

auc_roc_test_bert = roc_auc_score(y_true_bert_torch, y_pred_bert_torch)

accuracy_test_bert = accuracy_score(y_true_bert_torch,y_pred_bert_torch)

print(f'Test Accuracy: {accuracy_test_bert}')

print(f'Test Precision: {precision_test_bert}')
print(f'Test Recall: {recall_test_bert}')
print(f'Test F1 Score: {f1_test_bert}')

print(f"AUC-ROC Score: {auc_roc_test_bert}")

# Plotting the confusion matrix.

cm = confusion_matrix(y_true_bert_torch,y_pred_bert_torch)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Upheld', 'Not Upheld'])
disp.plot(cmap=plt.cm.Blues)
plt.title('Confusion Matrix for BERT Model')
plt.show()

# Classification report
print(classification_report(y_true_bert_torch, y_pred_bert_torch, target_names=label_encoder.classes_))